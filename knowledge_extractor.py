import os
import csv
import time
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai

# ==========================================
# âš™ï¸ ì„¤ì • (Configuration)
# ==========================================
# 1. ì´ê³³ì— ë°œê¸‰ë°›ìœ¼ì‹  Gemini API í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.
GEMINI_API_KEY = "AIzaSyCLGdaxUntcBOurJMM9CwnN8P2JX3DMPP0"

# 2. í¬ë¡¤ë§ì„ ì‹œì‘í•  ê²½ìƒë‚¨ë„ì‚¬íšŒì„œë¹„ìŠ¤ì› í˜ì´ì§€ ëª©ë¡ (ì˜ˆì‹œ)
# ì´ ë¦¬ìŠ¤íŠ¸ì— ì‚¬ì—… ì•ˆë‚´ í˜ì´ì§€ URLë“¤ì„ ê³„ì† ì¶”ê°€í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
TARGET_URLS = [
    "https://gn.pass.or.kr/sub04/sub01_01.php", # ê²½ìƒë‚¨ë„ì¥ì• ì¸ì¢…í•©ë³µì§€ê´€
    "https://gn.pass.or.kr/sub04/sub02_01.php", # ê²½ìƒë‚¨ë„ë³´ì¡°ê¸°ê¸°ì„¼í„°
    "https://gn.pass.or.kr/sub04/sub03_01.php", # ê²½ìƒë‚¨ë„í”¼í•´ì¥ì• ì¸ì‰¼í„°
    "https://gn.pass.or.kr/sub04/sub04_01.php", # ê²½ìƒë‚¨ë„ì²­ì–´ë¦°ì´ì§‘
]

# ê²°ê³¼ë¬¼ì´ ì €ì¥ë  ë¶„ë°° íŒŒì¼ ì´ë¦„
OUTPUT_CSV_FILE = "temp_ì§€ì‹ë² ì´ìŠ¤.csv"

# ==========================================

# ì œë¯¸ë‚˜ì´ AI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)

# Gemini ëª¨ë¸ ì„¤ì • (Gemini 1.5 Flash ê¶Œì¥: ë¹ ë¥´ê³  ì €ë ´í•˜ë©° í…ìŠ¤íŠ¸ ì²˜ë¦¬ì— ìš°ìˆ˜í•¨)
generation_config = {
  "temperature": 0.2, # ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
  "top_p": 0.95,
  "top_k": 64,
  "max_output_tokens": 8192,
}

model = genai.GenerativeModel(
  model_name="gemini-1.5-flash",
  generation_config=generation_config,
)

def scrape_page_content(url):
    """
    ì£¼ì–´ì§„ URLì— ì ‘ì†í•˜ì—¬ ë¹µë¶€ìŠ¤ëŸ¬ê¸°(ë©”ë‰´) ì¹´í…Œê³ ë¦¬ì™€ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¸ì–´ì˜µë‹ˆë‹¤.
    """
    print(f"ğŸŒ ì ‘ì† ì¤‘: {url}")
    try:
        # User-Agentë¥¼ ì„¤ì •í•˜ì—¬ ì›¹ì„œë²„ê°€ ë´‡ ì°¨ë‹¨ì„ í•˜ì§€ ì•Šë„ë¡ ë°©ì§€
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        response = requests.get(url, headers=headers)
        response.raise_for_status() # ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. ë¹µë¶€ìŠ¤ëŸ¬ê¸°(Breadcrumb) ì¶”ì¶œì„ í†µí•œ ëŒ€/ì¤‘/ì†Œë¶„ë¥˜ íŒŒì•…
        category_nav = soup.find('div', class_='path') # ì‹¤ì œ í™ˆí˜ì´ì§€ ì†ŒìŠ¤ì½”ë“œì— ë§ê²Œ ìˆ˜ì • í•„ìš”
        categories = []
        if category_nav:
            # ì˜ˆ: í™ˆ > ì£¼ìš”ì‚¬ì—… > ë¯¼ê°„ì§€ì› ì‚¬ì—… > ëŒ€ì²´ì¸ë ¥ì§€ì›
            categories = [item.text.strip() for item in category_nav.find_all('span') if item.text.strip() != '']
        
        # ë¶„ë¥˜ê°€ ì œëŒ€ë¡œ ì¡íˆì§€ ì•Šì•˜ì„ ê²½ìš° ê¸°ë³¸ê°’
        large_cat = categories[1] if len(categories) > 1 else 'ë¶„ë¥˜ì—†ìŒ'
        mid_cat = categories[2] if len(categories) > 2 else 'ë¶„ë¥˜ì—†ìŒ'
        small_cat = categories[3] if len(categories) > 3 else 'ë¶„ë¥˜ì—†ìŒ'

        # 2. ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # í™ˆí˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ë³¸ë¬¸ì„ ë‹´ê³  ìˆëŠ” tag(ex: div class="content_box")ë¥¼ ì§€ì •
        content_area = soup.find('div', id='sub') 
        raw_text = content_area.get_text(separator='\n', strip=True) if content_area else "ë³¸ë¬¸ ì—†ìŒ"
        
        return {
            "large_cat": large_cat,
            "mid_cat": mid_cat,
            "small_cat": small_cat,
            "raw_text": raw_text
        }
        
    except Exception as e:
        print(f"âŒ ì ‘ì† ì˜¤ë¥˜ ({url}): {e}")
        return None

def extract_with_gemini(scraped_data):
    """
    ê¸ì–´ì˜¨ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì œë¯¸ë‚˜ì´ì—ê²Œ ë„˜ê²¨ì„œ JSON(êµ¬ì¡°í™”ëœ ë°ì´í„°) í˜•íƒœë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    """
    print("ğŸ¤– ì œë¯¸ë‚˜ì´ í…ìŠ¤íŠ¸ ìš”ì•½ ë° êµ¬ì¡°í™” ì‹œì‘...")
    
    prompt = f"""
ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ê³µê³µê¸°ê´€(ê²½ìƒë‚¨ë„ì‚¬íšŒì„œë¹„ìŠ¤ì›)ì˜ AI ë§ì¶¤í˜• ë³µì§€ ë„¤ë¹„ê²Œì´í„°ì— íƒ‘ì¬ë  'ì§€ì‹ë² ì´ìŠ¤' êµ¬ì¶•ì„ ë‹´ë‹¹í•˜ëŠ” ì „ë¬¸ ë°ì´í„° ì—”ì§€ë‹ˆì–´ì´ì AI í•™ìŠµ ë°ì´í„° ì„¤ê³„ìì•¼.

ë‹¤ìŒì€ ê¸°ê´€ í™ˆí˜ì´ì§€ì˜ íŠ¹ì • ì‚¬ì—… í˜ì´ì§€ì—ì„œ ê¸ì–´ì˜¨ ì›ë¬¸ í…ìŠ¤íŠ¸ì•¼.
ì´ ì •ì œë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ë¥¼ ì½ê³ , ì•„ë˜ì˜ 6ê°€ì§€ í•µì‹¬ í•­ëª©ì— ë§ê²Œ ì™„ë²½í•˜ê²Œ ìš”ì•½ ë° ì¶”ì¶œí•´ì„œ **ë°˜ë“œì‹œ ìˆœìˆ˜í•œ JSON ë°°ì—´ í˜•ì‹**ìœ¼ë¡œë§Œ ë‹µë³€í•´ì¤˜. ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ë„£ì§€ ë§ˆ.

[ì›ë¬¸ í…ìŠ¤íŠ¸]
{scraped_data['raw_text']}

[ì¶”ì¶œí•´ì•¼ í•  6ê°€ì§€ í•µì‹¬ í•­ëª© (JSON í‚¤ ì´ë¦„)]
- "Title": ì‚¬ì—…ëª… (ë³µì§€ ì‚¬ì—…ì˜ ì •í™•í•œ ëª…ì¹­)
- "Tags": ì´ ì‚¬ì—…ì„ ì°¾ì„ ë²•í•œ ëŒ€ìƒìë‚˜ ìƒí™©ì„ ì¼ìƒì–´ í‚¤ì›Œë“œë¡œ 3~5ê°œ ì¶”ì¶œ (ì˜ˆ: "#ë…¸ì¸, #í˜¼ì, #ê±°ë™ë¶ˆí¸")
- "Eligibility": ì§€ì› ëŒ€ìƒ (ì—°ë ¹, ê¸°ì¤€, ì†Œë“ ìš”ê±´ ë“±)
- "Core_Benefits": ì§€ì› ë‚´ìš© (í˜„ê¸ˆ, ë¬¼í’ˆ, ë°©ë¬¸ ì„œë¹„ìŠ¤ ë“± í•µì‹¬ë§Œ ìš”ì•½)
- "How_to_Apply": ì‹ ì²­ ë°©ë²• ë° ì ˆì°¨ (ì¥ì†Œ, í•„ìš” ì„œë¥˜ ë“±)
- "Contact": ë‹´ë‹¹ ë¶€ì„œ ë° ë¬¸ì˜ì²˜ (ë¶€ì„œëª…, ì „í™”ë²ˆí˜¸)

* ì£¼ì˜ì‚¬í•­: 
1. ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ê³  ë¹ˆ ë¬¸ìì—´("")ë¡œ ë‚¨ê²¨ë‘˜ ê²ƒ.
2. ê²°ê³¼ë¬¼ì€ ë‹¨ 1ê°œì˜ JSON Objectë¥¼ ë‹´ì€ ë°°ì—´(`[ {{...}} ]`) í˜•íƒœë¡œ ì¶œë ¥í•  ê²ƒ. ì˜ˆë¥¼ ë“¤ì–´ ì›ë¬¸ì´ ì—¬ëŸ¬ ì‚¬ì—…ì„ ì„¤ëª…í•œë‹¤ë©´ ë°°ì—´ ì•ˆì— ì—¬ëŸ¬ ê°œì˜ Objectë¥¼ ë§Œë“¤ ê²ƒ.
    """

    try:
        response = model.generate_content(prompt)
        # ì œë¯¸ë‚˜ì´ì˜ ì‘ë‹µì—ì„œ JSONë¶€ë¶„ë§Œ ì¶”ì¶œ (ê°€ë” ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ëŸ­ì„ ì”Œìš°ëŠ” ê²½ìš° ëŒ€ë¹„)
        response_text = response.text.strip()
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
            
        return response_text.strip()
        
    except Exception as e:
        print(f"âŒ ì œë¯¸ë‚˜ì´ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def save_to_csv(structured_data_list, filename):
    """
    ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸´ ë°ì´í„°ë“¤ì„ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘: {filename}")
    
    # ì—‘ì…€ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ 'utf-8-sig' ì¸ì½”ë”© ì‚¬ìš©
    with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = ['ëŒ€ë¶„ë¥˜', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ì‚¬ì—…ëª…', 'í‚¤ì›Œë“œ íƒœê·¸', 'ì§€ì› ëŒ€ìƒ', 'ì§€ì› ë‚´ìš©', 'ì‹ ì²­ ë°©ë²•', 'ë¬¸ì˜ì²˜', 'ì¶œì²˜ URL']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        
        # ì œë¯¸ë‚˜ì´ê°€ ì¤€ JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ë°°ì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (eval í˜¹ì€ json.loads ì‚¬ìš©)
        import json
        
        for item in structured_data_list:
             try:
                 # ì œë¯¸ë‚˜ì´ ì‘ë‹µ í˜•íƒœê°€ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ì´ë¼ê³  ê°€ì •
                 json_objects = json.loads(item['gemini_response'])
                 
                 for obj in json_objects:
                     writer.writerow({
                        'ëŒ€ë¶„ë¥˜': item['large_cat'],
                        'ì¤‘ë¶„ë¥˜': item['mid_cat'],
                        'ì†Œë¶„ë¥˜': item['small_cat'],
                        'ì‚¬ì—…ëª…': obj.get('Title', ''),
                        'í‚¤ì›Œë“œ íƒœê·¸': obj.get('Tags', ''),
                        'ì§€ì› ëŒ€ìƒ': obj.get('Eligibility', ''),
                        'ì§€ì› ë‚´ìš©': obj.get('Core_Benefits', ''),
                        'ì‹ ì²­ ë°©ë²•': obj.get('How_to_Apply', ''),
                        'ë¬¸ì˜ì²˜': obj.get('Contact', ''),
                        'ì¶œì²˜ URL': item['url']
                     })
             except Exception as e:
                 print(f"JSON íŒŒì‹± ì—ëŸ¬ ë°œìƒ, í•´ë‹¹ ë°ì´í„° ìŠ¤í‚µ: {e}")
                 print(f"ì—ëŸ¬ ì›ì¸ í…ìŠ¤íŠ¸: {item['gemini_response']}")

def main():
    if GEMINI_API_KEY == "ì—¬ê¸°ì—_API_í‚¤ë¥¼_ì…ë ¥í•˜ì„¸ìš”":
        print("âš ï¸ ì˜¤ë¥˜: GEMINI_API_KEYë¥¼ ì½”ë“œ ìƒë‹¨ì— ì…ë ¥í•´ì£¼ì„¸ìš”!")
        return

    all_structured_data = []

    print("ğŸš€ ì§€ì‹ë² ì´ìŠ¤ ìë™ ë°ì´í„° ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    for url in TARGET_URLS:
        # 1. ì›¹í˜ì´ì§€ ê¸ì–´ì˜¤ê¸° (ì¹´í…Œê³ ë¦¬ + ë³¸ë¬¸)
        scraped_data = scrape_page_content(url)
        
        if scraped_data and scraped_data['raw_text'] != "ë³¸ë¬¸ ì—†ìŒ":
            # 2. ì œë¯¸ë‚˜ì´ì—ê²Œ ë˜ì ¸ì„œ A~Gì—´ ê·œê²©ì— ë§ëŠ” JSON ë¬¸ìì—´ ë½‘ì•„ë‚´ê¸°
            gemini_response = extract_with_gemini(scraped_data)
            
            if gemini_response:
                all_structured_data.append({
                    "url": url,
                    "large_cat": scraped_data['large_cat'],
                    "mid_cat": scraped_data['mid_cat'],
                    "small_cat": scraped_data['small_cat'],
                    "gemini_response": gemini_response
                })
            
            # ì„œë²„ ë¬´ë¦¬ë¥¼ ì£¼ì§€ ì•Šê¸° ìœ„í•´ 3ì´ˆ íœ´ì‹ (ë§¤ë„ˆ ë”œë ˆì´)
            time.sleep(3) 

    # 3. ì™„ì„±ëœ ë°ì´í„°ë¥¼ CSV ì—‘ì…€ë¡œ ì €ì¥í•˜ê¸°
    if all_structured_data:
        save_to_csv(all_structured_data, OUTPUT_CSV_FILE)
        print(f"ğŸ‰ ì„±ê³µ! {OUTPUT_CSV_FILE} íŒŒì¼ì´ ë°”íƒ•í™”ë©´(í˜„ì¬ í´ë”)ì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ¤·â€â™‚ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
